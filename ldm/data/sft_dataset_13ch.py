from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import imp

import os
from io import BytesIO
import json
import logging
import base64
from sys import prefix
import threading
import random
from turtle import left, right
import numpy as np
from typing import Any, Callable, List, Tuple, Union
from PIL import Image,ImageDraw
import torch.utils.data as data
import json
import time
import cv2
import torch
import torchvision
import torch.nn.functional as F
import torchvision.transforms as T
import copy
import math
from functools import partial
import albumentations as A
import bezier

import os.path as osp
from PIL import Image
from torch.utils.data import Dataset
import torchvision.transforms as transforms
import torchvision.transforms.functional as TF
from einops import rearrange
from torchvision.utils import save_image



import warnings
warnings.filterwarnings("ignore")


from torchvision.transforms import ToTensor, ToPILImage
# sys.path.append('/data/pzb/EAT/attack')
from thinplatespline.batch import TPS
from thinplatespline.tps import tps_warp
TOTEN = ToTensor()
TOPIL = ToPILImage()
DEVICE = torch.device("cpu")



def grid_points_2d(width, height, device=DEVICE):
    """
    Create 2d grid points. Distribute points across a width and height,
    with the resulting coordinates constrained to -1, 1
    returns tensor shape (width * height, 2)
    """
    xx, yy = torch.meshgrid(
        [torch.linspace(-1.0, 1.0, height, device=device),
         torch.linspace(-1.0, 1.0, width, device=device)])
    return torch.stack([yy, xx], dim=-1).contiguous().view(-1, 2)
def noisy_grid(width, height, noise_matrix, device=DEVICE):
    """
    Make uniform grid points, and add noise except for edge points.
    """
    grid = grid_points_2d(width, height, device)
    mod = torch.zeros([height, width, 2], device=device)
    mod[1:height - 1, 1:width - 1, :] = noise_matrix
    return grid + mod.reshape(-1, 2)
def grid_to_img(grid_points, width, height):
    """
    convert (N * 2) tensor of grid points in -1, 1 to tuple of (x, y)
    scaled to width, height.
    return (x, y) to plot"""
    grid_clone = grid_points.clone().detach().cpu().numpy()
    x = (1 + grid_clone[..., 0]) * (width - 1) / 2
    y = (1 + grid_clone[..., 1]) * (height - 1) / 2
    return x.flatten(), y.flatten()
def decow(img,scale=0.8):
    n, c, w, h = img.size()
    device = torch.device('cpu')
    a = 3
    X = grid_points_2d(a, a, device)
    noise = (torch.rand([a-2, a-2, 2]) - 0.5) * scale
    # noise = (torch.rand([1, 1, 2]) - 0.5)
    Y = noisy_grid(a, a, noise, device)
    tpsb = TPS(size=(h, w), device=device)
    warped_grid_b = tpsb(X[None, ...], Y[None, ...])
    warped_grid_b = warped_grid_b.repeat(img.shape[0], 1, 1, 1)
    awt_img = torch.grid_sampler_2d(img, warped_grid_b, 0, 0, False)
    return awt_img


def bbox_process(bbox):
    x_min = int(bbox[0])
    y_min = int(bbox[1])
    x_max = x_min + int(bbox[2])
    y_max = y_min + int(bbox[3])
    return list(map(int, [x_min, y_min, x_max, y_max]))


def get_tensor(normalize=True, toTensor=True):
    transform_list = []
    if toTensor:
        transform_list += [torchvision.transforms.ToTensor()]

    if normalize:
        transform_list += [torchvision.transforms.Normalize((0.5, 0.5, 0.5),
                                                (0.5, 0.5, 0.5))]
    return torchvision.transforms.Compose(transform_list)

def get_tensor_clip(normalize=True, toTensor=True):
    transform_list = []
    if toTensor:
        transform_list += [torchvision.transforms.ToTensor()]

    if normalize:
        transform_list += [torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073),
                                                (0.26862954, 0.26130258, 0.27577711))]
    return torchvision.transforms.Compose(transform_list)


#####

# 1:skin, 2:nose, 3:eye_g, 4:l_eye, 5:r_eye, 6:l_brow, 7:r_brow, 8:l_ear, 9:r_ear, 
# 10:mouth, 11:u_lip, 12:l_lip, 13:hair, 14:hat, 15:ear_r, 16:neck_l, 17:neck, 18:cloth

# 19 attributes in total, skin-1,nose-2,...cloth-18, background-0
celelbAHQ_label_list = ['skin', 'nose', 'eye_g', 'l_eye', 'r_eye',
                        'l_brow', 'r_brow', 'l_ear', 'r_ear', 'mouth',
                        'u_lip', 'l_lip', 'hair', 'hat', 'ear_r',
                        'neck_l', 'neck', 'cloth']

# face-parsing.PyTorch also includes 19 attributesï¼Œbut with different permutation
face_parsing_PyTorch_label_list = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye',
                                    'eye_g', 'l_ear', 'r_ear', 'ear_r', 'nose', 
                                    'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 
                                    'cloth', 'hair', 'hat']  # skin-1 l_brow-2 ...
 
# 9 attributes with left-right aggrigation
faceParser_label_list = ['background', 'mouth', 'eyebrows', 'eyes', 'hair', 
                         'nose', 'skin', 'ears', 'belowface']

# 12 attributes with left-right aggrigation
faceParser_label_list_detailed = ['background', 'lip', 'eyebrows', 'eyes', 'hair', 
                                  'nose', 'skin', 'ears', 'belowface', 'mouth', 
                                  'eye_glass', 'ear_rings']

TO_TENSOR = transforms.ToTensor()
MASK_CONVERT_TF = transforms.Lambda(
    lambda celebAHQ_mask: __celebAHQ_masks_to_faceParser_mask(celebAHQ_mask))

MASK_CONVERT_TF_DETAILED = transforms.Lambda(
    lambda celebAHQ_mask: __celebAHQ_masks_to_faceParser_mask_detailed(celebAHQ_mask))


NORMALIZE = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))

def un_norm_clip(x1):
    x = x1*1.0 # to avoid changing the original tensor or clone() can be used
    reduce=False
    if len(x.shape)==3:
        x = x.unsqueeze(0)
        reduce=True
    x[:,0,:,:] = x[:,0,:,:] * 0.26862954 + 0.48145466
    x[:,1,:,:] = x[:,1,:,:] * 0.26130258 + 0.4578275
    x[:,2,:,:] = x[:,2,:,:] * 0.27577711 + 0.40821073
    
    if reduce:
        x = x.squeeze(0)
    return x

    
def un_norm(x):
    return (x+1.0)/2.0

def get_transforms(normalize=True, toTensor=True):
    transform_list = []
    if toTensor:
        transform_list += [transforms.ToTensor()]

    if normalize:
        transform_list += [transforms.Normalize((0.5, 0.5, 0.5),
                                                (0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)



def __celebAHQ_masks_to_faceParser_mask_detailed(celebA_mask):
    """Convert the semantic image of CelebAMaskHQ to reduced categories (12-class). 

    Args:
        mask (PIL image): with shape [H,W]
    Return:
        aggrigated mask, with same shape [H,W] but the number of segmentation classes is less
    """
    # 19 attributes in total, skin-1,nose-2,...cloth-18, background-0
    celelbAHQ_label_list = ['skin', 'nose', 'eye_g', 'l_eye', 'r_eye',
                            'l_brow', 'r_brow', 'l_ear', 'r_ear', 'mouth',
                            'u_lip', 'l_lip', 'hair', 'hat', 'ear_r',
                            'neck_l', 'neck', 'cloth']# 12 attributes with left-right aggrigation
    faceParser_label_list_detailed = ['background', 'lip', 'eyebrows', 'eyes', 'hair', 
                                    'nose', 'skin', 'ears', 'belowface', 'mouth', 
                                  'eye_glass', 'ear_rings']

    converted_mask = np.zeros_like(celebA_mask)

    backgorund = np.equal(celebA_mask, 0)
    converted_mask[backgorund] = 0

    lip = np.logical_or(np.equal(celebA_mask, 11), np.equal(celebA_mask, 12))
    converted_mask[lip] = 1

    eyebrows = np.logical_or(np.equal(celebA_mask, 6),
                             np.equal(celebA_mask, 7))
    converted_mask[eyebrows] = 2

    eyes = np.logical_or(np.equal(celebA_mask, 4), np.equal(celebA_mask, 5))
    converted_mask[eyes] = 3

    hair = np.equal(celebA_mask, 13)
    converted_mask[hair] = 4

    nose = np.equal(celebA_mask, 2)
    converted_mask[nose] = 5

    skin = np.equal(celebA_mask, 1)
    # print('skin', np.sum(skin))
    converted_mask[skin] = 6

    ears = np.logical_or(np.equal(celebA_mask, 8), np.equal(celebA_mask, 9))
    converted_mask[ears] = 7

    belowface = np.equal(celebA_mask, 17)
    converted_mask[belowface] = 8
    
    mouth = np.equal(celebA_mask, 10)   
    converted_mask[mouth] = 9

    eye_glass = np.equal(celebA_mask, 3)
    converted_mask[eye_glass] = 10
    
    ear_rings = np.equal(celebA_mask, 15)
    converted_mask[ear_rings] = 11
    
    return converted_mask

def __celebAHQ_masks_to_faceParser_mask(celebA_mask):
    """Convert the semantic image of CelebAMaskHQ to reduced categories (9-class). 

    Args:
        mask (PIL image): with shape [H,W]
    Return:
        aggrigated mask, with same shape [H,W] but the number of segmentation classes is less
    """

    assert len(celebA_mask.size) == 2, "The provided mask should be with [H,W] format"

    converted_mask = np.zeros_like(celebA_mask)

    backgorund = np.equal(celebA_mask, 0)
    converted_mask[backgorund] = 0

    mouth = np.logical_or(
        np.logical_or(np.equal(celebA_mask, 10), np.equal(celebA_mask, 11)),
        np.equal(celebA_mask, 12)
    )
    converted_mask[mouth] = 1

    eyebrows = np.logical_or(np.equal(celebA_mask, 6),
                             np.equal(celebA_mask, 7))
    converted_mask[eyebrows] = 2

    eyes = np.logical_or(np.equal(celebA_mask, 4), np.equal(celebA_mask, 5))
    converted_mask[eyes] = 3

    hair = np.equal(celebA_mask, 13)
    converted_mask[hair] = 4

    nose = np.equal(celebA_mask, 2)
    converted_mask[nose] = 5

    skin = np.equal(celebA_mask, 1)
    converted_mask[skin] = 6

    ears = np.logical_or(np.equal(celebA_mask, 8), np.equal(celebA_mask, 9))
    converted_mask[ears] = 7

    belowface = np.equal(celebA_mask, 17)
    converted_mask[belowface] = 8

    return converted_mask




class CelebAdataset(data.Dataset):
    def __init__(self,state,arbitrary_mask_percent=0,load_vis_img=False,label_transform=None,fraction=1.0,**args
        ):
        self.label_transform=label_transform
        self.fraction=fraction
        self.load_vis_img=load_vis_img
        self.state=state
        self.args=args
        self.arbitrary_mask_percent=arbitrary_mask_percent
        self.kernel = np.ones((1, 1), np.uint8)
        self.random_trans=A.Compose([
            A.Resize(height=224,width=224),
            A.HorizontalFlip(p=0.5),
            A.Rotate(limit=20),
            A.Blur(p=0.3),
            A.ElasticTransform(p=0.3), 
            # A.GaussNoise(p=0.3),# newly added from this line
            # A.HueSaturationValue(p=0.3),
            # A.ISONoise(p=0.3),
            # A.Solarize(p=0.3),
            ])
        
        self.gray_outer_mask=args['gray_outer_mask']
        # self.preserve=args['preserve_mask']
        if hasattr(args, 'preserve_mask'):
            self.preserve=args['preserve_mask']
            self.remove_tar=args['preserve_mask']
            self.preserve_src=args['preserve_mask']
        else:
            self.preserve=args['preserve_mask_src']
            self.remove_tar=args['remove_mask_tar']
            self.preserve_src=args['preserve_mask_src']
        
        
        self.Fullmask=False
        
        self.bbox_path_list=[]
        if state == "train":
            self.imgs = sorted([osp.join(args['dataset_dir'], "CelebA-HQ-img", "%d.jpg"%idx) for idx in range(28000)])
            # self.labels = ([osp.join(self.root, "CelebA-HQ-mask", "%d"%int(idx/2000) ,'{0:0=5d}'.format(idx)+'_skin.png') for idx in range(28000)])
            self.labels =  sorted([osp.join(args['dataset_dir'], "CelebA-HQ-mask/Overall_mask", "%d.png"%idx) for idx in range(28000)]) 
            self.labels_vis =  sorted([osp.join(args['dataset_dir'], "vis", "%d.png"%idx) for idx in range(28000)]) if self.load_vis_img else None
        elif state == "validation":
            self.imgs = sorted([osp.join(args['dataset_dir'], "CelebA-HQ-img", "%d.jpg"%idx) for idx in range(28000, 30000)])
            # self.labels = ([osp.join(self.root, "CelebA-HQ-mask", "%d"%int(idx/2000) ,'{0:0=5d}'.format(idx)+'_skin.png') for idx in range(28000, 30000)])
            self.labels =  sorted([osp.join(args['dataset_dir'], "CelebA-HQ-mask/Overall_mask", "%d.png"%idx) for idx in range(28000, 30000)]) 
            self.labels_vis =  sorted([osp.join(args['dataset_dir'], "vis", "%d.png"%idx) for idx in range(28000, 30000)]) if self.load_vis_img else None
        else:
            self.imgs = sorted([osp.join(args['dataset_dir'], "CelebA-HQ-img", "%d.jpg"%idx) for idx in range(28000, 30000)])
            # self.labels = ([osp.join(self.root, "CelebA-HQ-mask", "%d"%int(idx/2000) ,'{0:0=5d}'.format(idx)+'_skin.png') for idx in range(28000, 30000)])
            self.labels =  sorted([osp.join(args['dataset_dir'], "CelebA-HQ-mask/Overall_mask", "%d.png"%idx) for idx in range(28000, 30000)]) 
            self.labels_vis =  sorted([osp.join(args['dataset_dir'], "vis", "%d.png"%idx) for idx in range(28000, 30000)]) if self.load_vis_img else None
        
        self.imgs= self.imgs[:int(len(self.imgs)*self.fraction)]
        self.labels= self.labels[:int(len(self.labels)*self.fraction)]
        self.labels_vis= self.labels_vis[:int(len(self.labels_vis)*self.fraction)]  if self.load_vis_img else None

        if self.load_vis_img:
            assert len(self.imgs) == len(self.labels) == len(self.labels_vis)
        else:
            assert len(self.imgs) == len(self.labels)

        # image pairs indices
        self.indices = np.arange(len(self.imgs))
        self.length=len(self.indices)

    def __getitem__(self, index):
        if self.gray_outer_mask:
            return self.__getitem_gray__(index)
        else:
            return self.__getitem_black__(index)


    def __getitem_gray__(self, index):

        img_path = self.imgs[index]
        img_p = Image.open(img_path).convert('RGB')
     

        ############
        mask_path = self.labels[index]
        mask_img = Image.open(mask_path).convert('L')
        
        if self.Fullmask:
            mask_img_full=mask_img
            mask_img_full=get_tensor(normalize=False, toTensor=True)(mask_img_full)
        
        mask_img = np.array(mask_img)  # Convert the label to a NumPy array if it's not already
        
        
            
        
        # Create a mask to preserve values in the 'preserve' list
        # preserve = [1,2,4,5,8,9,17 ]
        # preserve = [1,2,4,5,8,9 ]
        preserve = self.preserve # full mask to be changed
        mask = np.isin(mask_img, preserve)

        # Create a converted_mask where preserved values are set to 255
        converted_mask = np.zeros_like(mask_img)
        converted_mask[mask] = 255
        # convert to PIL image
        mask_img=Image.fromarray(converted_mask).convert('L')
        mask_tensor=1-get_tensor(normalize=False, toTensor=True)(mask_img)
 
 

        if self.load_vis_img:
            label_vis = self.labels_vis[index]
            label_vis = Image.open(label_vis).convert('RGB')
            label_vis = TO_TENSOR(label_vis)
        else:
            label_vis = -1  # unified interface
        
    
        img_p_np=cv2.imread(img_path)
        img_p_np = cv2.cvtColor(img_p_np, cv2.COLOR_BGR2RGB)
        ref_image_tensor=img_p_np
        # resize mask_img
       
    
        
        # ref_image_tensor=self.random_trans(image=ref_image_tensor)
        ref_image_tensor=Image.fromarray(ref_image_tensor)
        ref_image_tensor=get_tensor_clip()(ref_image_tensor)
       

        ### Generate mask
        image_tensor = get_tensor()(img_p)
        W,H = img_p.size

        image_tensor_cropped=image_tensor
        mask_tensor_cropped=mask_tensor
        image_tensor_resize=T.Resize([self.args['image_size'],self.args['image_size']])(image_tensor_cropped)
        mask_tensor_resize=T.Resize([self.args['image_size'],self.args['image_size']])(mask_tensor_cropped)
        
        # a=random.randint(1,4)
        scale=random.uniform(0.5, 1.0)
        mask_tensor_resize=decow(mask_tensor_resize.unsqueeze(0) ,scale=scale).squeeze(0)
        inpaint_tensor_resize=image_tensor_resize*mask_tensor_resize
        
        mask_ref=1-T.Resize([1024,1024])(mask_tensor)
        ref_image_tensor=ref_image_tensor*mask_ref
        
        # ref_image_tensor=Image.fromarray(ref_image_tensor)
        ref_image_tensor=255.* rearrange(un_norm_clip(ref_image_tensor), 'c h w -> h w c').cpu().numpy()
        
        ref_image_tensor=self.random_trans(image=ref_image_tensor)
        ref_image_tensor=Image.fromarray(ref_image_tensor['image'].astype(np.uint8)) 
        ref_image_tensor=get_tensor_clip()(ref_image_tensor)
   
        if self.Fullmask:
            return {"GT":image_tensor_resize,"inpaint_image":inpaint_tensor_resize,"inpaint_mask":mask_img_full,"ref_imgs":ref_image_tensor}
   
        return {"GT":image_tensor_resize,"inpaint_image":inpaint_tensor_resize,"inpaint_mask":mask_tensor_resize,"ref_imgs":ref_image_tensor}

    def __getitem_black__(self, index):
        # black mask
        img_path = self.imgs[index]
        img_p = Image.open(img_path).convert('RGB')
     

        ############
        mask_path = self.labels[index]
        mask_img = Image.open(mask_path).convert('L')
        mask_img = np.array(mask_img)  # Convert the label to a NumPy array if it's not already

        # Create a mask to preserve values in the 'preserve' list
        # preserve = [1,2,4,5,8,9,17 ]
        # preserve = [1,2,4,5,8,9 ]
        preserve = self.preserve # full mask to be changed
        mask = np.isin(mask_img, preserve)

        # Create a converted_mask where preserved values are set to 255
        converted_mask = np.zeros_like(mask_img)
        converted_mask[mask] = 255
        # convert to PIL image
        mask_img=Image.fromarray(converted_mask).convert('L')
        mask_tensor=1-get_tensor(normalize=False, toTensor=True)(mask_img)
 
 

        if self.load_vis_img:
            label_vis = self.labels_vis[index]
            label_vis = Image.open(label_vis).convert('RGB')
            label_vis = TO_TENSOR(label_vis)
        else:
            label_vis = -1  # unified interface
        
    
        img_p_np=cv2.imread(img_path)
        img_p_np = cv2.cvtColor(img_p_np, cv2.COLOR_BGR2RGB)
        ref_image_tensor=img_p_np
        # resize mask_img
        mask_img_r = mask_img.resize(img_p_np.shape[1::-1], Image.NEAREST)
        mask_img_r = np.array(mask_img_r)
        
        # select only mask_img region from reference image
        ref_image_tensor[mask_img_r==0]=0   # comment this if full img should be used
    
        
        ref_image_tensor=self.random_trans(image=ref_image_tensor)
        ref_image_tensor=Image.fromarray(ref_image_tensor["image"])
        ref_image_tensor=get_tensor_clip()(ref_image_tensor)



        ### Generate mask
        image_tensor = get_tensor()(img_p)
        W,H = img_p.size

        image_tensor_cropped=image_tensor
        mask_tensor_cropped=mask_tensor
        image_tensor_resize=T.Resize([self.args['image_size'],self.args['image_size']])(image_tensor_cropped)
        mask_tensor_resize=T.Resize([self.args['image_size'],self.args['image_size']])(mask_tensor_cropped)
        inpaint_tensor_resize=image_tensor_resize*mask_tensor_resize
   
        return {"GT":image_tensor_resize,"inpaint_image":inpaint_tensor_resize,"inpaint_mask":mask_tensor_resize,"ref_imgs":ref_image_tensor}
   
   
    def __getitem_old__(self, index):

        
        img_path = self.imgs[index]
        img_p = Image.open(img_path).convert('RGB')
        # if self.img_transform is not None:
        #     img = self.img_transform(img)

        label = self.labels[index]
        label = Image.open(label).convert('L')
        # Assuming that 'label' is your binary mask (black and white image)
        label = np.array(label)  # Convert the label to a NumPy array if it's not already

        # Find the coordinates of the non-zero (white) pixels in the mask
        non_zero_coords = np.column_stack(np.where(label == 1))

        # Find the minimum and maximum x and y coordinates to get the bounding box
        min_x, min_y = np.min(non_zero_coords, axis=0)
        max_x, max_y = np.max(non_zero_coords, axis=0)

        # Add padding if needed
        padding = 0
        min_x = max(0, min_x - padding)
        min_y = max(0, min_y - padding)
        max_x = min(img_p.size[0], max_x + padding)
        max_y = min(img_p.size[1], max_y + padding)

        # The bounding box coordinates are now (min_x, min_y, max_x, max_y)
        # Scale the bounding box coordinates to match the image size (1024x1024)
        min_x *= 2
        min_y *= 2
        max_x *= 2
        max_y *= 2
        bbox = [min_x, min_y, max_x, max_y]
        
        if self.label_transform is not None:
            label= self.label_transform(label)
 

        if self.load_vis_img:
            label_vis = self.labels_vis[index]
            label_vis = Image.open(label_vis).convert('RGB')
            label_vis = TO_TENSOR(label_vis)
        else:
            label_vis = -1  # unified interface
        
        # img_p, label, label_vis = self.load_single_image(index)
        # bbox=[30,50,60,100]
   
        ### Get reference image
        bbox_pad=copy.copy(bbox)
        bbox_pad[0]=bbox[0]-min(10,bbox[0]-0)
        bbox_pad[1]=bbox[1]-min(10,bbox[1]-0)
        bbox_pad[2]=bbox[2]+min(10,img_p.size[0]-bbox[2])
        bbox_pad[3]=bbox[3]+min(10,img_p.size[1]-bbox[3])
        img_p_np=cv2.imread(img_path)
        img_p_np = cv2.cvtColor(img_p_np, cv2.COLOR_BGR2RGB)
        ref_image_tensor=img_p_np[bbox_pad[1]:bbox_pad[3],bbox_pad[0]:bbox_pad[2],:]
        ref_image_tensor=self.random_trans(image=ref_image_tensor)
        ref_image_tensor=Image.fromarray(ref_image_tensor["image"])
        ref_image_tensor=get_tensor_clip()(ref_image_tensor)



        ### Generate mask
        image_tensor = get_tensor()(img_p)
        W,H = img_p.size

        extended_bbox=copy.copy(bbox)
        left_freespace=bbox[0]-0
        right_freespace=W-bbox[2]
        up_freespace=bbox[1]-0
        down_freespace=H-bbox[3]
        extended_bbox[0]=bbox[0]-random.randint(0,int(0.4*left_freespace))
        extended_bbox[1]=bbox[1]-random.randint(0,int(0.4*up_freespace))
        extended_bbox[2]=bbox[2]+random.randint(0,int(0.4*right_freespace))
        extended_bbox[3]=bbox[3]+random.randint(0,int(0.4*down_freespace))

        prob=random.uniform(0, 1)
        if prob<self.arbitrary_mask_percent:
            mask_img = Image.new('RGB', (W, H), (255, 255, 255)) 
            bbox_mask=copy.copy(bbox)
            extended_bbox_mask=copy.copy(extended_bbox)
            top_nodes = np.asfortranarray([
                            [bbox_mask[0],(bbox_mask[0]+bbox_mask[2])/2 , bbox_mask[2]],
                            [bbox_mask[1], extended_bbox_mask[1], bbox_mask[1]],
                        ])
            down_nodes = np.asfortranarray([
                    [bbox_mask[2],(bbox_mask[0]+bbox_mask[2])/2 , bbox_mask[0]],
                    [bbox_mask[3], extended_bbox_mask[3], bbox_mask[3]],
                ])
            left_nodes = np.asfortranarray([
                    [bbox_mask[0],extended_bbox_mask[0] , bbox_mask[0]],
                    [bbox_mask[3], (bbox_mask[1]+bbox_mask[3])/2, bbox_mask[1]],
                ])
            right_nodes = np.asfortranarray([
                    [bbox_mask[2],extended_bbox_mask[2] , bbox_mask[2]],
                    [bbox_mask[1], (bbox_mask[1]+bbox_mask[3])/2, bbox_mask[3]],
                ])
            top_curve = bezier.Curve(top_nodes,degree=2)
            right_curve = bezier.Curve(right_nodes,degree=2)
            down_curve = bezier.Curve(down_nodes,degree=2)
            left_curve = bezier.Curve(left_nodes,degree=2)
            curve_list=[top_curve,right_curve,down_curve,left_curve]
            pt_list=[]
            random_width=5
            for curve in curve_list:
                x_list=[]
                y_list=[]
                for i in range(1,19):
                    if (curve.evaluate(i*0.05)[0][0]) not in x_list and (curve.evaluate(i*0.05)[1][0] not in y_list):
                        pt_list.append((curve.evaluate(i*0.05)[0][0]+random.randint(-random_width,random_width),curve.evaluate(i*0.05)[1][0]+random.randint(-random_width,random_width)))
                        x_list.append(curve.evaluate(i*0.05)[0][0])
                        y_list.append(curve.evaluate(i*0.05)[1][0])
            mask_img_draw=ImageDraw.Draw(mask_img)
            mask_img_draw.polygon(pt_list,fill=(0,0,0))
            mask_tensor=get_tensor(normalize=False, toTensor=True)(mask_img)[0].unsqueeze(0)
        else:
            mask_img=np.zeros((H,W))
            mask_img[extended_bbox[1]:extended_bbox[3],extended_bbox[0]:extended_bbox[2]]=1
            mask_img=Image.fromarray(mask_img)
            mask_tensor=1-get_tensor(normalize=False, toTensor=True)(mask_img)

        ### Crop square image
        if W > H:
            left_most=extended_bbox[2]-H
            if left_most <0:
                left_most=0
            right_most=extended_bbox[0]+H
            if right_most > W:
                right_most=W
            right_most=right_most-H
            if right_most<= left_most:
                image_tensor_cropped=image_tensor
                mask_tensor_cropped=mask_tensor
            else:
                left_pos=random.randint(left_most,right_most) 
                free_space=min(extended_bbox[1]-0,extended_bbox[0]-left_pos,left_pos+H-extended_bbox[2],H-extended_bbox[3])
                random_free_space=random.randint(0,int(0.6*free_space))
                image_tensor_cropped=image_tensor[:,0+random_free_space:H-random_free_space,left_pos+random_free_space:left_pos+H-random_free_space]
                mask_tensor_cropped=mask_tensor[:,0+random_free_space:H-random_free_space,left_pos+random_free_space:left_pos+H-random_free_space]
        
        elif  W < H:
            upper_most=extended_bbox[3]-W
            if upper_most <0:
                upper_most=0
            lower_most=extended_bbox[1]+W
            if lower_most > H:
                lower_most=H
            lower_most=lower_most-W
            if lower_most<=upper_most:
                image_tensor_cropped=image_tensor
                mask_tensor_cropped=mask_tensor
            else:
                upper_pos=random.randint(upper_most,lower_most) 
                free_space=min(extended_bbox[1]-upper_pos,extended_bbox[0]-0,W-extended_bbox[2],upper_pos+W-extended_bbox[3])
                random_free_space=random.randint(0,int(0.6*free_space))
                image_tensor_cropped=image_tensor[:,upper_pos+random_free_space:upper_pos+W-random_free_space,random_free_space:W-random_free_space]
                mask_tensor_cropped=mask_tensor[:,upper_pos+random_free_space:upper_pos+W-random_free_space,random_free_space:W-random_free_space]
        else:
            image_tensor_cropped=image_tensor
            mask_tensor_cropped=mask_tensor

        image_tensor_resize=T.Resize([self.args['image_size'],self.args['image_size']])(image_tensor_cropped)
        mask_tensor_resize=T.Resize([self.args['image_size'],self.args['image_size']])(mask_tensor_cropped)
        inpaint_tensor_resize=image_tensor_resize*mask_tensor_resize
        
        # save_image(image_tensor_resize, "Train_data_images/"+str(index)+'_image_tensor_resize.png')
        # save_image(inpaint_tensor_resize, "Train_data_images/"+ str(index)+'_inpaint_tensor_resize.png')
        # save_image(mask_tensor_resize, "Train_data_images/"+ str(index)+'_mask_tensor_resize.png')
        # save_image(ref_image_tensor,  "Train_data_images/"+str(index)+'_ref_image_tensor.png')
        
        return {"GT":image_tensor_resize,"inpaint_image":inpaint_tensor_resize,"inpaint_mask":mask_tensor_resize,"ref_imgs":ref_image_tensor}


    def __len__(self):
        return self.length
    
    

#####SFTFaceDataset########################



import os
import json
import cv2
import torch
import random
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import torchvision.transforms as transforms
import torchvision.transforms as T
from torch.utils import data
import albumentations as A
from einops import rearrange


try:
    import models.third_party.model_resnet_d3dfr as model_resnet_d3dfr
    import models.third_party.d3dfr.bfm as bfm
except ImportError:
    print("Warning: D3DFR modules not found. Ensure 'third_party' is in python path.")    


def draw_landmarks_on_black(size, landmarks, radius=4):
    """
    åœ¨å…¨é»‘èƒŒæ™¯ä¸Šç»˜åˆ¶å½©è™¹è‰²å…³é”®ç‚¹
    Args:
        size: å›¾ç‰‡è¾¹é•¿ (int)
        landmarks: (68, 2) numpy array
        radius: ç‚¹çš„åŠå¾„
    """
    # åˆ›å»ºé»‘è‰²åº•å›¾ (H, W, 3)
    img_draw = np.zeros((size, size, 3), dtype=np.uint8)
    
    # å½©è™¹è‰²ç”Ÿæˆ
    colors = plt.get_cmap('rainbow')(np.linspace(0, 1, len(landmarks)))
    colors = (255 * colors).astype(int)[:, :3].tolist()
    
    for i, (x, y) in enumerate(landmarks):
        # è¾¹ç•Œæ£€æŸ¥
        if x < 0 or x >= size or y < 0 or y >= size:
            continue
            
        color = colors[i]
        #ä»¥æ­¤é¢œè‰²å¡«å……åœ†: OpenCVä½¿ç”¨BGRï¼Œæˆ‘ä»¬è¿™é‡Œç”Ÿæˆçš„æ˜¯RGBï¼Œç¨åç»Ÿä¸€è½¬
        # plt.get_cmap ç”Ÿæˆçš„æ˜¯ RGBA, æˆ‘ä»¬å– RGB. cv2.circle éœ€è¦ color æ˜¯ int tuple
        cv2.circle(img_draw, (int(x), int(y)), radius=radius, 
                  color=(color[0], color[1], color[2]), thickness=-1) # ä½¿ç”¨RGBé¡ºåºï¼Œå› ä¸ºåç»­è½¬PIL
    
    return img_draw


class SFTFaceDataset(data.Dataset):
    def __init__(self, data_manifest_path, base_3d_path,args, **kwargs):
        super().__init__()
        with open(data_manifest_path, "r") as f:
            self.data_list = json.load(f)
        self.args = args
        # ä» args ä¸­è¯»å– image_sizeï¼Œä¸ config æ–‡ä»¶ä¸€è‡´
        self.img_size = int(args.get("image_size", 512)) 

        # åŸºç¡€å˜æ¢
        self.to_img = get_tensor(normalize=True, toTensor=True)      
        self.to_mask = get_tensor(normalize=False, toTensor=True)    
        self.to_clip = get_tensor_clip() # å³ get_tensor_clip()
        self.resize_img = T.Resize([self.img_size, self.img_size], interpolation=T.InterpolationMode.BILINEAR)
        self.resize_mask = T.Resize([self.img_size, self.img_size], interpolation=T.InterpolationMode.NEAREST)

        # ref_imgs çš„å¢å¼º
        self.random_trans = A.Compose([
            A.Resize(height=224, width=224),
            A.HorizontalFlip(p=0.5),
            A.Rotate(limit=20),
            A.Blur(p=0.2),
        ])

        # è¯­ä¹‰ mask çš„ä¿ç•™ç±»åˆ« - æ”¯æŒæ ¹æ®æ•°æ®ç±»å‹ï¼ˆsft/reconï¼‰åŠ¨æ€é€‰æ‹©
        # æ¨¡å¼1: å…¼å®¹æ¨¡å¼ï¼ˆæ—§é…ç½®ï¼‰
        if 'preserve_mask' in args:
            self.preserve_src_sft = args['preserve_mask']
            self.remove_tar_sft = args['preserve_mask']
            self.preserve_src_recon = args['preserve_mask']
            self.remove_tar_recon = args['preserve_mask']
            self.dynamic_mask = False
            print(f"[SFTFaceDataset] Using legacy mode with single mask config: {self.preserve_src_sft}")
        
        # æ¨¡å¼2: ç»Ÿä¸€é…ç½®ï¼ˆsftå’Œreconä½¿ç”¨ç›¸åŒé…ç½®ï¼‰
        elif 'preserve_mask_src' in args and 'remove_mask_tar' in args:
            self.preserve_src_sft = args['preserve_mask_src']
            self.remove_tar_sft = args['remove_mask_tar']
            self.preserve_src_recon = args['preserve_mask_src']
            self.remove_tar_recon = args['remove_mask_tar']
            self.dynamic_mask = False
            print(f"[SFTFaceDataset] Using unified mode - Source: {self.preserve_src_sft}, Target: {self.remove_tar_sft}")
        
        # æ¨¡å¼3: åŠ¨æ€é…ç½®ï¼ˆsftå’Œreconä½¿ç”¨ä¸åŒé…ç½®ï¼‰â­ æ–°å¢
        elif 'preserve_mask_src_sft' in args:
            self.preserve_src_sft = args['preserve_mask_src_sft']
            self.remove_tar_sft = args['remove_mask_tar_sft']
            self.preserve_src_recon = args['preserve_mask_src_recon']
            self.remove_tar_recon = args['remove_mask_tar_recon']
            self.dynamic_mask = True
            print(f"[SFTFaceDataset] Using dynamic mode:")
            print(f"  - SFT    -> Source: {self.preserve_src_sft}, Target: {self.remove_tar_sft}")
            print(f"  - Recon  -> Source: {self.preserve_src_recon}, Target: {self.remove_tar_recon}")
        else:
            raise ValueError("Error: Must provide mask configurations. See comments for supported modes.")

        if 'ref_imgs_augmentation' in args:
            self.ref_imgs_augmentation = args['ref_imgs_augmentation']
        else:
            self.ref_imgs_augmentation = False
        if 'un_norm_clip' not in globals() or 'rearrange' not in globals():
            raise ImportError("Error: 'un_norm_clip' or 'rearrange' function not found.")

        print(">>> [SFTFaceDataset] Loading 3D models (D3DFR & BFM)...")
        self.device_3d = 'cpu' 
        # D3DFR éœ€è¦çš„è¾“å…¥é¢„å¤„ç†
        self.d3dfr_transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=0.5, std=0.5)
        ])

        # åŠ è½½ D3DFR
        d3dfr_path = os.path.join(base_3d_path, 'd3dfr_res50_nofc.pth')
        self.net_d3dfr = model_resnet_d3dfr.getd3dfr_res50(d3dfr_path).eval().to(self.device_3d)
        
        # åŠ è½½ BFM
        bfm_path = os.path.join(base_3d_path, 'BFM_model_front.mat')
        self.bfm_facemodel = bfm.BFM(
            focal=1015*256/224,     
            image_size=256, 
            bfm_model_path=bfm_path
        ).to(self.device_3d)
        print(">>> [SFTFaceDataset] 3D Models loaded.")

    def __len__(self):
        return len(self.data_list)

    def get_3d_mixed_landmark_map(self, src_pil, tgt_pil):
        """
         æ‰§è¡Œ 3D æå–ã€æ··åˆå‚æ•°ã€ç”Ÿæˆå…³é”®ç‚¹å¹¶åœ¨é»‘è‰²åº•å›¾ä¸Šç»˜åˆ¶
        """
        src_tensor = self.d3dfr_transform(src_pil).unsqueeze(0).to(self.device_3d)
        tgt_tensor = self.d3dfr_transform(tgt_pil).unsqueeze(0).to(self.device_3d)
        with torch.no_grad():
            # æå–ç³»æ•°
            src_coeff = self.net_d3dfr(src_tensor) # (1, 257)
            tgt_coeff = self.net_d3dfr(tgt_tensor) # (1, 257)

            # æ··åˆç³»æ•°: Source ID (0:80) + Target Exp/Texture (80:)
            mixed_coeff = tgt_coeff.clone()
            mixed_coeff[:, 0:80] = src_coeff[:, 0:80]

            # ç”Ÿæˆ 68 ä¸ªå…³é”®ç‚¹ (åŸºäº 256x256 ç©ºé—´)
            # get_lm68 è¿”å› (B, 68, 2)
            mixed_pts68_network = self.bfm_facemodel.get_lm68(mixed_coeff)[0].cpu().numpy()

        # åæ ‡æ˜ å°„: ä» 256 æ˜ å°„åˆ° self.img_size (ä¾‹å¦‚ 512)
        scale = self.img_size / 256.0
        mixed_pts_final = mixed_pts68_network * scale

        # åœ¨é»‘è‰²åº•å›¾ä¸Šç»˜åˆ¶
        radius = max(2, int(self.img_size / 128)) 
        black_bg_img = draw_landmarks_on_black(self.img_size, mixed_pts_final, radius=radius)
        return Image.fromarray(black_bg_img)

    def _build_inpaint_from_mask(self, mask_pil, preserve_list=None, return_before_augment=False):
            
        mask_np = np.array(mask_pil)
        keep = np.isin(mask_np, preserve_list)
        converted = np.zeros_like(mask_np, dtype=np.uint8)
        converted[keep] = 255
        mask_keep_pil = Image.fromarray(converted).convert("L")
        mask_keep_tensor = self.to_mask(mask_keep_pil) 
        inpaint_mask = 1.0 - mask_keep_tensor
        inpaint_mask = self.resize_mask(inpaint_mask)
        
        # ä¿å­˜å¢å¼ºå‰çš„ mask
        inpaint_mask_before = inpaint_mask.clone()
        
        # åº”ç”¨ TPS å½¢å˜å¢å¼º
        scale = random.uniform(0.5, 1.0)
        inpaint_mask = decow(inpaint_mask.unsqueeze(0), scale=scale).squeeze(0)
        
        if return_before_augment:
            return inpaint_mask, inpaint_mask_before
        return inpaint_mask

    def __getitem__(self, idx):
        s = self.data_list[idx]

        label = s.get("label", "sft")  # é»˜è®¤ä¸º'sft'
        
        # â­ æ ¹æ®labelåŠ¨æ€é€‰æ‹©maské…ç½®
        if label == 'recon':
            preserve_src = self.preserve_src_recon
            remove_tar = self.remove_tar_recon
        else:  # 'sft'
            preserve_src = self.preserve_src_sft
            remove_tar = self.remove_tar_sft

        # 1. è¯»å–æ‰€æœ‰ PIL å›¾åƒ
        ref_pil      = Image.open(s["path_B_source"]).convert("RGB")
        ref_mask_pil = Image.open(s["path_B_mask"]).convert("L")   # Source Mask
        tgt_mask_pil = Image.open(s["path_D_mask"]).convert("L")   # Target Mask
        
        win_pil      = Image.open(s["path_A_chosen"]).convert("RGB")
        tgt_pil      = Image.open(s.get("path_D_target")) if s.get("path_D_target") else None
        if tgt_pil:
            tgt_pil = tgt_pil.convert("RGB")

        
        # A. åˆ›å»ºä¸€ä¸ªç”¨äº Tensors çš„ Resize å˜æ¢
        tensor_resize_op = T.Resize([self.img_size, self.img_size])

        # B. (Tensor) å…ˆå°† *å®Œæ•´å°ºå¯¸* PIL è½¬ä¸º Tensor
        GT_w_full_tensor = self.to_img(win_pil)
        base_pil = tgt_pil if tgt_pil is not None else win_pil
        base_img_full_tensor = self.to_img(base_pil)

        # C. (Resize) å†å¯¹ Tensor è¿›è¡Œ Resize
        GT_w = tensor_resize_op(GT_w_full_tensor)
        base_img = tensor_resize_op(base_img_full_tensor)
        
        ref_pil_224 = ref_pil.resize((224, 224), Image.BILINEAR)
        ref_img_raw = self.to_clip(ref_pil_224)  # Bå›¾åƒï¼ˆæœªmaskï¼Œ224x224ï¼ŒCLIPå½’ä¸€åŒ–ï¼‰
        tgt_img_raw = base_img  # Då›¾åƒï¼ˆæœªmaskï¼Œ512x512ï¼Œæ ‡å‡†å½’ä¸€åŒ–ï¼‰
        
        # è·å–å¢å¼ºå‰åçš„mask - ä½¿ç”¨åŠ¨æ€é€‰æ‹©çš„ remove_tar
        inpaint_mask_augmented, inpaint_mask_before_augment = self._build_inpaint_from_mask(
            tgt_mask_pil, 
            preserve_list=remove_tar,  # â­ ä½¿ç”¨åŠ¨æ€é€‰æ‹©çš„é…ç½®
            return_before_augment=True
        ) 
        
        # æ ¹æ®labelå†³å®šä½¿ç”¨å“ªä¸ªmask
        # å¦‚æœæ˜¯reconä»»åŠ¡ï¼Œä½¿ç”¨å¢å¼ºåçš„maskï¼ˆå¢åŠ æ•°æ®å¤šæ ·æ€§ï¼‰
        # å¦‚æœæ˜¯sftä»»åŠ¡ï¼Œä½¿ç”¨æœªå¢å¼ºçš„maskï¼ˆä¿æŒç²¾ç¡®å¯¹åº”ï¼‰
        if label == 'recon':
            inpaint_mask = inpaint_mask_augmented
        else:
            inpaint_mask = inpaint_mask_before_augment
        
        inpaint_image = base_img * inpaint_mask
        
        
        # A. (ä½¿ç”¨ ref_mask_pil) åˆ›å»ºäºŒå€¼ mask (0/255 numpy)
        # ä½¿ç”¨ preserve_srcï¼šä¿ç•™æ ¸å¿ƒäººè„¸åŒºåŸŸï¼Œç”¨äºèº«ä»½ç‰¹å¾æå–
        ref_mask_np = np.array(ref_mask_pil)
        keep_ref = np.isin(ref_mask_np, preserve_src)  # â­ ä½¿ç”¨åŠ¨æ€é€‰æ‹©çš„é…ç½®
        converted_ref_np = np.zeros_like(ref_mask_np, dtype=np.uint8)
        converted_ref_np[keep_ref] = 255
        
        # B. è½¬æ¢ä¸º mask_tensor (0.0/1.0 Tensor, 1=æ´)
        mask_tensor_for_ref = 1.0 - self.to_mask(Image.fromarray(converted_ref_np).convert('L'))
        
        # C. ç”¨ BILINEAR ç¼©æ”¾ mask_tensor åˆ° ref_pil çš„å°ºå¯¸
        resized_mask_tensor = T.Resize(ref_pil.size, interpolation=T.InterpolationMode.BILINEAR)(mask_tensor_for_ref)
        
        # D. åˆ›å»º mask_ref (0.0/1.0 Tensor, 0=æ´, 1=ä¿ç•™)
        mask_ref = 1.0 - resized_mask_tensor
        
        # E. ã€Norm 1ã€‘åœ¨ *æœªé®ç½©* çš„ ref_pil ä¸Šè¿›è¡Œç¬¬ä¸€æ¬¡å½’ä¸€åŒ–
        tensor_clip = self.to_clip(ref_pil)
        
        # F. ã€Maskã€‘å°† mask åº”ç”¨äº *å·²å½’ä¸€åŒ–* çš„ tensor
        tensor_clip = tensor_clip * mask_ref
        
        # G. ã€Un-Normã€‘
        tensor_unclip = un_norm_clip(tensor_clip)
        np_unclip_255 = 255. * rearrange(tensor_unclip, 'c h w -> h w c').cpu().numpy()
        

        # === ğŸ‘‡ æ ¸å¿ƒä¿®æ”¹å¼€å§‹ ğŸ‘‡ ===
        
        # 1. ç¡®ä¿è½¬æˆ uint8 æ ¼å¼ (Albumentations éœ€è¦è¿™ä¸ªæ ¼å¼)
        np_img_face = np_unclip_255.astype(np.uint8)

        # 2. æ‰§è¡Œ Ref å¢å¼º (å¦‚æœå¼€å…³æ‰“å¼€)
        if self.ref_imgs_augmentation:
            # ç›´æ¥æŠŠå›¾ç‰‡æ‰”è¿›å»å˜æ¢
            # å› ä¸ºèƒŒæ™¯å·²ç»è¢« Mask å˜æˆäº†é»‘è‰²(0)ï¼Œæ—‹è½¬æ—¶å¡«é»‘è¾¹æ˜¯å®‰å…¨çš„
            augmented = self.random_trans(image=np_img_face)
            np_img_face = augmented['image']

        # 3. è½¬å› PIL å¹¶ Resize åˆ° 224 (CLIP çš„è¾“å…¥å°ºå¯¸)
        # æ­¤æ—¶çš„ np_img_face å·²ç»æ˜¯å¢å¼ºè¿‡ï¼ˆæ¯”å¦‚ç¿»è½¬è¿‡ï¼‰çš„äº†
        ref_pil_processed = Image.fromarray(np_img_face)
        ref_pil_processed = ref_pil_processed.resize((224, 224), Image.BILINEAR)
        
        # 4. ã€Norm 2ã€‘æœ€ç»ˆå½’ä¸€åŒ–åˆ°CLIPæ ¼å¼
        ref_imgs = self.to_clip(ref_pil_processed)
        
        # === ğŸ‘† æ ¸å¿ƒä¿®æ”¹ç»“æŸ ğŸ‘† ===


        # -----------------------------------------------------------------
        # 3. [æ–°å¢] ç”Ÿæˆ 3D Landmark Guide Map
        # -----------------------------------------------------------------
        # è¾“å…¥: Source (ref_pil) å’Œ Target (base_pil)
        # è¾“å‡º: é»‘è‰²èƒŒæ™¯ + æ··åˆåçš„å½©è™¹è‰²å…³é”®ç‚¹ PIL
        mixed_lm_pil = self.get_3d_mixed_landmark_map(ref_pil, base_pil)
        
        # è½¬ä¸º Tensor å¹¶å½’ä¸€åŒ–åˆ° [-1, 1] (ä¸ GT, inpaint_image æ ¼å¼ä¸€è‡´)
        mixed_3d_landmarks = self.to_img(mixed_lm_pil)
        
        
        out = {
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # 1. GT_w çœŸæ­£çš„ç›‘ç£å€¼
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            "GT_w": GT_w,
            # æ¥æº: path_A_chosen (Aå›¾åƒ)
            # å†…å®¹: å¥½çš„ä¿®å¤ç»“æœï¼ˆæ¢è„¸åçš„æ­£ç¡®ç»“æœï¼‰
            # å½¢çŠ¶: (3, 512, 512)
            # å½’ä¸€åŒ–: æ ‡å‡†å½’ä¸€åŒ– [-1, 1], mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)
            # maskçŠ¶æ€: æœªmaskï¼ˆå®Œæ•´å›¾åƒï¼‰
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # GT target,è‡ªç›‘ç£å½“ä¸­ä¹Ÿæ˜¯gt,è¿™é‡Œä¿ç•™åå­—äº†
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            "GT": base_img,
            # æ¥æº: path_D_target
            # å†…å®¹: ç›®æ ‡å›¾åƒï¼ˆæä¾›å§¿æ€å’ŒèƒŒæ™¯ï¼‰
            # å½¢çŠ¶: (3, 512, 512)
            # å½’ä¸€åŒ–: æ ‡å‡†å½’ä¸€åŒ– [-1, 1]
            # maskçŠ¶æ€: æœªmaskï¼ˆå®Œæ•´å›¾åƒï¼‰
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # inpaint_image (æ¨¡å‹çš„ä¸»è¦è¾“å…¥)
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            "inpaint_image": inpaint_image,
            # æ¥æº: base_img * inpaint_mask
            # å†…å®¹: targetå›¾åƒè¢«maské®ç½©åçš„ç»“æœï¼ˆè„¸éƒ¨è¢«é®ç½©å˜é»‘ï¼ŒèƒŒæ™¯ä¿ç•™ï¼‰ï¼Œå¦‚æœæ˜¯reconä»»åŠ¡ï¼Œåˆ™ä½¿ç”¨å¢å¼ºåçš„maskï¼Œå¦‚æœæ˜¯sftä»»åŠ¡ï¼Œåˆ™ä½¿ç”¨æœªå¢å¼ºçš„mask
            # å½¢çŠ¶: (3, 512, 512)
            # å½’ä¸€åŒ–: æ ‡å‡†å½’ä¸€åŒ– [-1, 1]
            # maskçŠ¶æ€: å·²maskï¼ˆè„¸éƒ¨åŒºåŸŸå€¼ä¸º0ï¼ŒèƒŒæ™¯åŒºåŸŸä¿ç•™åŸå€¼ï¼‰
            # å…·ä½“: inpaint_maskä¸­ 1.0çš„åœ°æ–¹ä¿ç•™åŸå›¾ï¼Œ0.0çš„åœ°æ–¹å˜ä¸º0ï¼ˆé»‘è‰²ï¼‰

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # inpaint_mask ï¼Œå¦‚æœæ˜¯reconä»»åŠ¡ï¼Œåˆ™ä½¿ç”¨å¢å¼ºåçš„maskï¼Œå¦‚æœæ˜¯sftä»»åŠ¡ï¼Œåˆ™ä½¿ç”¨æœªå¢å¼ºçš„mask
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            "inpaint_mask": inpaint_mask,
            # æ¥æº: _build_inpaint_from_mask(å¢å¼ºå‰ç‰ˆæœ¬)
            # å†…å®¹: äºŒå€¼maskï¼ˆè§„åˆ™è¾¹ç•Œï¼Œæœªç»TPSå½¢å˜ï¼‰
            # å½¢çŠ¶: (1, 512, 512)
            # æ•°å€¼èŒƒå›´: [0.0, 1.0]
            # maskè¯­ä¹‰: 1.0 = ä¿ç•™åŒºåŸŸï¼ˆèƒŒæ™¯ï¼‰ï¼Œ0.0 = é®ç½©åŒºåŸŸï¼ˆéœ€è¦ä¿®å¤çš„è„¸éƒ¨ï¼‰
            
        
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # ref_imgs (å‚è€ƒå›¾åƒ - å¸¦maskï¼Œç”¨äºæ¡ä»¶ç¼–ç )
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            "ref_imgs": ref_imgs,
            # æ¥æº: sourceå›¾åƒç»è¿‡å¤æ‚çš„mask+å½’ä¸€åŒ–å¤„ç†
            # å†…å®¹: sourceå›¾åƒåªä¿ç•™æ ¸å¿ƒäººè„¸åŒºåŸŸï¼ˆskin,nose,eyes,brows,mouth,lipsï¼‰
            #       éæ ¸å¿ƒåŒºåŸŸï¼ˆears,neckï¼‰è¢«maskæ‰ï¼ˆå€¼ä¸º0ï¼‰
            # å½¢çŠ¶: (3, 224, 224)
            # å½’ä¸€åŒ–: CLIPå½’ä¸€åŒ–
            # æ•°å€¼èŒƒå›´: å…¸å‹ [-2.0, 2.0]
            # maskçŠ¶æ€: å·²maskï¼ˆåªä¿ç•™æ ¸å¿ƒäººè„¸ï¼Œéæ ¸å¿ƒåŒºåŸŸä¸º0ï¼‰
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # 9. ref_img_raw (å‚è€ƒå›¾åƒ - æœªmask)
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            "ref_img_raw": ref_img_raw,
            # æ¥æº: Bæºå›¾åƒç›´æ¥resizeå’Œå½’ä¸€åŒ–
            # å†…å®¹: Bçš„å®Œæ•´å›¾åƒï¼ˆåŒ…æ‹¬æ‰€æœ‰åŒºåŸŸï¼šäººè„¸+è€³æœµ+è„–å­+èƒŒæ™¯ï¼‰
            # å½¢çŠ¶: (3, 224, 224)
            # å½’ä¸€åŒ–: CLIPå½’ä¸€åŒ–
            # æ•°å€¼èŒƒå›´: å…¸å‹ [-2.0, 2.0]
            # maskçŠ¶æ€: æœªmaskï¼ˆå®Œæ•´å›¾åƒï¼‰
            
     

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # mixed_3d_landmarks (3D å…³é”®ç‚¹æ··åˆå¼•å¯¼å›¾) 
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            "mixed_3d_landmarks": mixed_3d_landmarks
            # æ¥æº: D3DFRæ¨¡å‹æå– source ID + target Pose æ··åˆåç”Ÿæˆçš„å…³é”®ç‚¹
            # å†…å®¹: é»‘è‰²èƒŒæ™¯ä¸Šç»˜åˆ¶çš„å½©è™¹è‰²å…³é”®ç‚¹
            # å½¢çŠ¶: (3, 512, 512)
            # å½’ä¸€åŒ–: æ ‡å‡†å½’ä¸€åŒ– [-1, 1]
            # ä½œç”¨: ä½œä¸ºå¼ºå‡ ä½•å¼•å¯¼ (Geometric Guidance)ï¼Œè¾…åŠ©æ¨¡å‹å¯¹é½äº”å®˜ä½ç½®
        }
            
        return out
########################DPOFaceDataset########################


    
  