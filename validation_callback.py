"""
éªŒè¯é˜¶æ®µè‡ªåŠ¨ç”Ÿæˆå›¾ç‰‡å¹¶è¯„ä¼°çš„å›è°ƒ
æ”¯æŒï¼š
1. åœ¨æŒ‡å®šglobal_stepåè§¦å‘éªŒè¯
2. åªç”Ÿæˆå‰Nå¯¹éªŒè¯å›¾ç‰‡ï¼ˆé»˜è®¤100å¯¹ï¼‰
3. è‡ªåŠ¨è°ƒç”¨è¯„ä¼°è„šæœ¬è®¡ç®—æŒ‡æ ‡
"""

import os
import torch
import numpy as np
from pathlib import Path
from PIL import Image
import subprocess
from pytorch_lightning.callbacks import Callback
from pytorch_lightning.utilities.distributed import rank_zero_only


class ValidationEvaluationCallback(Callback):
    """
    åœ¨éªŒè¯é˜¶æ®µç”Ÿæˆå›¾ç‰‡å¹¶è‡ªåŠ¨è¯„ä¼°
    """
    def __init__(
        self, 
        eval_every_n_steps=300,      # æ¯éš”å¤šå°‘æ­¥è¿›è¡Œä¸€æ¬¡è¯„ä¼°
        max_val_samples=100,          # æœ€å¤šè¯„ä¼°å¤šå°‘å¯¹å›¾ç‰‡
        batch_size=4,                 # ç”Ÿæˆæ—¶çš„batch size
        ddim_steps=50,                # DDIMé‡‡æ ·æ­¥æ•°
        save_dir=None,                # ä¿å­˜ç›®å½•ï¼ŒNoneåˆ™ä½¿ç”¨checkpointç›®å½•
        run_evaluation=True,          # æ˜¯å¦è¿è¡Œè¯„ä¼°è„šæœ¬
        device=0,                     # è¯„ä¼°ä½¿ç”¨çš„GPU
        start_step=0,                 # ä»å“ªä¸ªstepå¼€å§‹è¯„ä¼°
    ):
        super().__init__()
        self.eval_every_n_steps = eval_every_n_steps
        self.max_val_samples = max_val_samples
        self.batch_size = batch_size
        self.ddim_steps = ddim_steps
        self.save_dir = save_dir
        self.run_evaluation = run_evaluation
        self.device = device
        self.start_step = start_step
        self.last_eval_step = -1
        
    def should_evaluate(self, global_step):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœ¨å½“å‰æ­¥æ•°è¿›è¡Œè¯„ä¼°"""
        if global_step < self.start_step:
            return False
        if global_step == self.last_eval_step:
            return False
        if (global_step % self.eval_every_n_steps) == 0:
            return True
        return False
    
    @rank_zero_only
    def on_validation_epoch_end(self, trainer, pl_module):
        """éªŒè¯epochç»“æŸæ—¶è§¦å‘è¯„ä¼°"""
        global_step = pl_module.global_step
        
        if not self.should_evaluate(global_step):
            return
            
        print(f"\n{'='*80}")
        print(f"ğŸ” Starting Validation Evaluation at step {global_step}")
        print(f"{'='*80}\n")
        
        self.last_eval_step = global_step
        
        # 1. ç”ŸæˆéªŒè¯å›¾ç‰‡
        output_dir = self._generate_validation_images(trainer, pl_module, global_step)
        
        # 2. è¿è¡Œè¯„ä¼°
        if self.run_evaluation and output_dir is not None:
            self._run_evaluation_scripts(output_dir, global_step)
    
    def _generate_validation_images(self, trainer, pl_module, global_step):
        """ç”ŸæˆéªŒè¯é›†å›¾ç‰‡"""
        print(f"ğŸ“¸ Generating validation images (max {self.max_val_samples} samples)...")
        
        # è®¾ç½®ä¿å­˜ç›®å½•
        if self.save_dir is None:
            log_dir = Path(trainer.logger.save_dir)
            output_dir = log_dir / f"validation_eval_step_{global_step:06d}"
        else:
            output_dir = Path(self.save_dir) / f"step_{global_step:06d}"
        
        output_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ Output directory: {output_dir}")
        
        # è·å–éªŒè¯æ•°æ®åŠ è½½å™¨
        val_dataloader = trainer.val_dataloaders[0] if trainer.val_dataloaders else None
        if val_dataloader is None:
            print("âš ï¸  No validation dataloader found!")
            return None
        
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        was_training = pl_module.training
        if was_training:
            pl_module.eval()
        
        # ç”Ÿæˆå›¾ç‰‡
        total_generated = 0
        max_batches = (self.max_val_samples + self.batch_size - 1) // self.batch_size
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(val_dataloader):
                if batch_idx >= max_batches:
                    break
                
                try:
                    # å¤„ç†æ•°æ®æ ¼å¼
                    # test_bench_dataset è¿”å›å…ƒç»„: (image, prior, dict, index)
                    if isinstance(batch, (tuple, list)) and len(batch) == 4:
                        image_tensor, prior_tensor, batch_dict, index_str = batch
                        # æ„å»ºç¬¦åˆ log_images æœŸæœ›çš„å­—å…¸æ ¼å¼
                        batch = {
                            'image': image_tensor.to(pl_module.device),
                            'GT': prior_tensor.to(pl_module.device),
                            'inpaint_image': batch_dict['inpaint_image'].to(pl_module.device),
                            'inpaint_mask': batch_dict['inpaint_mask'].to(pl_module.device),
                            'ref': batch_dict['ref_imgs'].squeeze(1).to(pl_module.device),  # å»æ‰å¤šä½™ç»´åº¦
                        }
                    # å¤„ç†å­—å…¸æ ¼å¼ï¼ˆDPOæ•°æ®é›†ï¼‰
                    elif isinstance(batch, dict):
                        batch = {k: v.to(pl_module.device) if isinstance(v, torch.Tensor) else v 
                                for k, v in batch.items()}
                    
                    # ç”Ÿæˆå›¾ç‰‡ï¼ˆä½¿ç”¨EMAæ¨¡å‹ï¼‰
                    with pl_module.ema_scope("Validation"):
                        images_dict = pl_module.log_images(
                            batch,
                            N=min(batch['image'].shape[0], self.batch_size),
                            sample=True,
                            ddim_steps=self.ddim_steps,
                            ddim_eta=1.0
                        )
                    
                    # ä¿å­˜ç”Ÿæˆçš„å›¾ç‰‡
                    if 'output_current' in images_dict:
                        output_images = images_dict['output_current']
                    elif 'samples' in images_dict:
                        output_images = images_dict['samples']
                    else:
                        print(f"âš ï¸  No output images in batch {batch_idx}")
                        continue
                    
                    batch_size_actual = output_images.shape[0]
                    for idx in range(batch_size_actual):
                        if total_generated >= self.max_val_samples:
                            break
                        
                        # ä¿å­˜å›¾ç‰‡ï¼ˆæ ¼å¼ï¼š000000000000.pngï¼‰
                        img_path = output_dir / f"{total_generated:012d}.png"
                        self._save_image(output_images[idx], img_path)
                        total_generated += 1
                    
                    print(f"  Progress: {total_generated}/{self.max_val_samples} images generated", end='\r')
                    
                except Exception as e:
                    print(f"\nâš ï¸  Error in batch {batch_idx}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
        
        print(f"\nâœ… Generated {total_generated} validation images")
        
        # æ¢å¤è®­ç»ƒæ¨¡å¼
        if was_training:
            pl_module.train()
        
        return output_dir
    
    def _save_image(self, tensor, path):
        """ä¿å­˜å•å¼ å›¾ç‰‡"""
        # tensor: [C, H, W], range [-1, 1]
        img = tensor.cpu().numpy()
        img = (img + 1.0) / 2.0  # [-1, 1] -> [0, 1]
        img = np.transpose(img, (1, 2, 0))  # [C, H, W] -> [H, W, C]
        img = np.clip(img * 255, 0, 255).astype(np.uint8)
        Image.fromarray(img).save(path)
    
    def _run_evaluation_scripts(self, output_dir, global_step):
        """è¿è¡Œè¯„ä¼°è„šæœ¬"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š Running Evaluation Scripts")
        print(f"{'='*80}\n")
        
        # æ•°æ®è·¯å¾„ï¼ˆæ ¹æ®ä½ çš„é…ç½®ï¼‰
        source_path = "dataset/FaceData/CelebAMask-HQ/Val_target"
        target_path = "dataset/FaceData/CelebAMask-HQ/Val"
        source_mask_path = "dataset/FaceData/CelebAMask-HQ/target_mask"
        target_mask_path = "dataset/FaceData/CelebAMask-HQ/src_mask"
        dataset_path = "dataset/FaceData/CelebAMask-HQ/CelebA-HQ-img"
        
        results_file = output_dir / "evaluation_results.txt"
        
        # å†™å…¥è¯„ä¼°ä¿¡æ¯å¤´éƒ¨
        with open(results_file, 'w') as f:
            f.write(f"Evaluation Results\n")
            f.write(f"="*80 + "\n")
            f.write(f"Global Step: {global_step}\n")
            f.write(f"Output Directory: {output_dir}\n")
            f.write(f"Max Samples: {self.max_val_samples}\n")
            f.write(f"="*80 + "\n\n")
        
        env = os.environ.copy()
        env['CUDA_VISIBLE_DEVICES'] = str(self.device)
        
        # 1. FID Score
        print("ğŸ“ˆ Computing FID score...")
        self._run_metric_evaluation(
            "FID Score",
            ["python", "eval_tool/fid/fid_score.py",
             "--device", "cuda",
             "--max-samples", str(self.max_val_samples),
             dataset_path, str(output_dir)],
            results_file, env
        )
        
        # 2. ID Similarity
        print("ğŸ‘¤ Computing ID similarity...")
        self._run_metric_evaluation(
            "ID Similarity (Arcface)",
            ["python", "eval_tool/ID_retrieval/ID_retrieval.py",
             "--device", "cuda",
             "--max-samples", str(self.max_val_samples),
             "--dataset", "ffhq",
             "--arcface", "True",
             source_path, str(output_dir), source_mask_path, target_mask_path],
            results_file, env
        )
        
        # 3. Pose Comparison
        print("ğŸ¤¸ Computing pose consistency...")
        self._run_metric_evaluation(
            "Pose Comparison",
            ["python", "eval_tool/Pose/pose_compare.py",
             "--device", "cuda",
             "--max-samples", str(self.max_val_samples),
             target_path, str(output_dir)],
            results_file, env
        )
        
        # 4. Expression Comparison
        print("ğŸ˜Š Computing expression consistency...")
        self._run_metric_evaluation(
            "Expression Comparison",
            ["python", "eval_tool/Expression/expression_compare_face_recon.py",
             "--device", "cuda",
             "--max-samples", str(self.max_val_samples),
             target_path, str(output_dir)],
            results_file, env
        )
        
        print(f"\n{'='*80}")
        print(f"âœ… Evaluation Complete!")
        print(f"ğŸ“„ Results saved to: {results_file}")
        print(f"{'='*80}\n")
    
    def _run_metric_evaluation(self, metric_name, cmd, results_file, env):
        """è¿è¡Œå•ä¸ªæŒ‡æ ‡çš„è¯„ä¼°"""
        try:
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                env=env,
                timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
            )
            
            with open(results_file, 'a') as f:
                f.write(f"\n{metric_name}:\n")
                f.write("-" * 80 + "\n")
                f.write(result.stdout + "\n")
                if result.stderr:
                    f.write("Stderr:\n" + result.stderr + "\n")
            
            # æå–å…³é”®æ•°å€¼å¹¶æ‰“å°
            output_lines = result.stdout.strip().split('\n')
            for line in output_lines[-3:]:  # æ‰“å°æœ€å3è¡Œï¼ˆé€šå¸¸åŒ…å«ç»“æœï¼‰
                print(f"  {line}")
                
        except subprocess.TimeoutExpired:
            print(f"  âš ï¸  Timeout computing {metric_name}")
            with open(results_file, 'a') as f:
                f.write(f"\n{metric_name}: TIMEOUT\n")
        except Exception as e:
            print(f"  âš ï¸  Error computing {metric_name}: {e}")
            with open(results_file, 'a') as f:
                f.write(f"\n{metric_name}: ERROR - {e}\n")

